<!DOCTYPE html>
<html>
<head>
<title>CME 241: Reinforcement Learning for Stochastic Control Problems in Finance </title>
</head>

<body>
<h1>Welcome to CME241: Reinforcement Learning for Stochastic Control Problems in Finance</h1>
<h1>Instructor: <a href="mailto:ashwin.rao@stanford.edu">Ashwin Rao</a></h1>
<h2>Winter 2019 Classes: Wed & Fri 4:30pm - 5:50pm. <a href="https://campus-map.stanford.edu/?srch=200-203">Bldg 200 (Lane History Corner), Room 203</a>. CA: <a href="mailto:jeffgu@stanford.edu">Jeffrey Gu</a>.</h2>
<h2>Course Overview</h2>
<ul>
<li>Theory of Markov Decision Processes (MDP)</li>
<li>Dynamic Programming (DP) Algorithms</li>
<li>Reinforcement Learning (RL) Algorithms</li>
<li>We will model and apply these algorithms to 3 Financial/Trading problems:
<ul>
<li>(Dynamic) Asset-Allocation to maximize <em>Utility of Consumption</em></li>
<li>Optimal Exercise/Stopping of Path-dependent American Options</li>
<li> Optimal Trade Order Execution (managing <em>Price Impact</em>)</li>
</ul>
</li>
<li>By treating each of these problems as MDPs (i.e., Stochastic Control)</li>
<li>We will first go over classical/analytical solutions to these problems (in simple settings)</li>
<li>Then we will introduce real-world considerations, and tackle with RL (or DP)</li>
<li>Emphasis on Python implementations of these models and algorithms</li>
<li>More details on the course contents <a href="course_details.pdf">here</a></li>
</ul>
<h2>Pre-requisites</h2>
<ul>
<li> <strong>Required</strong>: Undergraduate-level background in Applied Mathematics, especially in Linear Algebra, Probability Theory and Optimization</li>
<li> <strong>Required</strong>: Background in Data Structures & Algorithms, with programming experience in numpy/scipy</li>
<li> <strong>Recommended</strong>: Basic background in Pricing and Portfolio Theory, although we will do an overview of the requisite Finance/Economics</li>
<li> <strong>Not required</strong>: MDP, DP, RL (we will cover these topics from scratch)</li>
</ul>
<h2>Grade will be based on</h2>
<ul>
	<li>Mid-Term Exam</li>
	<li>Final Exam</li>
	<li>Programming work (throughout the course)</li>
	<li>Class Participation</li>
</ul>
<h2>Learning Material will be a combination of</h2>
<ul>
	<li><a href="lecture_slides/">Technical Documents/Lecture Slides I have prepared specifically for this course</a></li>
	<li><a href="https://github.com/coverdrive/MDP-DP-RL">Python codebase I have developed for this course</a> to help you "learn through coding"</li>
	<li><a href="http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching.html">Slides and Videos from David Silver's UCL course on RL</a></li>
	<li>For deeper self-study and reference, augment the above content with <a href="http://incompleteideas.net/book/the-book-2nd.html">The Sutton-Barto RL Book and Sutton's accompanying teaching material</a></li>
</ul>
<h2>Learning Tenets</h2>
<ul>
	<li>When learning Theory, blend notational rigor with intuitive understanding</li>
	<li>Write down the mathematical formalism of the lecture contents with great precision (without refering to how I've written them)</li>
	<li>Always start with a simple model/simple algorithm and be clear about why you might want to use a more complex model/algorithm</li>
	<li>Programming is a powerful tool to grasp mathematical concepts (code all ideas/models/algorithms with the aim to visualize as well as validate mathematical properties)</li>
	<li>Code modularity/re-use is important (leverage Object-Oriented and Functional Programming paradigms)</li>
	<li>Important to recognize and then model various frictions encountered in real-world trading</li>
	<li>Discuss the ideas you learn with other students, with the CA, and with me. Question everything!</li>
	<li>Recognize that this is an introductory course. Learn the foundations slowly and thoroughly, before graduating to advanced RL (other courses we offer at Stanford)</li>
</ul>

<h2>Lecture-by-Lecture (tentative) schedule</h2>
<ul>
	<li>Week 1, Day 1: Course Overview - Intro to Markov Decision Processes, Reinforcement Learning and Finance Application Problems</li>
	<ul>
		<li>Lecture Slides: <a href="course_details.pdf">Course Overview</a>
			<li>Reading: First (Introduction) chapter of Sutton-Barto book (pages 1-12), <a href="lecture_slides/rich_sutton_slides/1-admin-and-intro.pdf">Sutton's slides on Intro to RL</a></li>
	</ul>
	<li>Week 1, Day 2: Markov Decision Processes</li>
	<ul>
		<li>Lecture Slides: <a href="lecture_slides/david_silver_slides/MDP.pdf">Markov Decision Processes</a></li>
		<li>Reading: Third (MDP) chapter of Sutton Barto book (pages 47-67), <a href="lecture_slides/rich_sutton_slides/5-6-MDPs.pdf">Sutton's slides on MDPs</a></li>
		<li>Assignment: Write out the MP/MRP/MDP/Value Function definitions(in TeX),  Code up Python classes for MP/MRP/MDPs/Policies and Python functions for Value Function calculations</li>
	</ul>

	<li>Week 2, Day 1: Bellman Equations</li>
	<ul>
		<li>Lecture Slides: <a href="lecture_slides/david_silver_slides/MDP.pdf">Markov Decision Processes</a></li>
		<li>Reading: Third (MDP) chapter of Sutton Barto book (pages 47-67), <a href="lecture_slides/rich_sutton_slides/5-6-MDPs.pdf">Sutton's slides on MDPs</a></li>
		<li>Assignment: Write out all the Bellman Equations (in TeX), Code up Python functions for Bellman calculations</li>
	</ul>
	<li>Week 2, Day 2: Dynamic Programming Algorithms</li>
	<li>Week 3, Day 1: Understanding Risk-Aversion through Utility Theory (as a pre-req to Application Problem 1)</li>
	<li>Week 3, Day 2: Application Problem 1 - Optimal Asset Allocation/Consumption (Merton's 1969 Paper)</li>
	<li>Week 4, Day 1: Application Problem 1 - Optimal Asset Allocation/Consumption (Discussion on Real-World problem)</li>
	<li>Week 4, Day 2: Application Problem 2 - Optimal Exercise of American Options</li>
	<li>Week 5, Day 1: Application Problem 3 - Optimal Trade Order Execution</li>
	<li>Week 5, Day 2: Mid-Term Exam</li>
	<li>Week 6: Model-free Prediction (RL for Value Function Estimation)</li>
	<li>Week 7: Model-Free Control (RL for Optimal Value Function/Policy)</li>
	<li>Week 8: RL with Function Approximation (including Deep RL)</li>
	<li>Week 9: Batch Methods (DQN, LSTDQ/LSPI), and Gradient TD</li>
	<li>Week 10: Policy Gradient Algorithms</li>
	<li>Week 11: Final Exam
</ul>
</body>
</html>

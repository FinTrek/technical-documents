<!DOCTYPE html>
<html>
<head>
<title>CME 241: Reinforcement Learning for Stochastic Control Problems in Finance </title>
</head>

<body>
<h1>Welcome to CME 241: Reinforcement Learning for Stochastic Control Problems in Finance</h1>
<h1>Instructor: <a href="mailto:ashwin.rao@stanford.edu">Ashwin Rao</a></h1>
<h2>Winter 2019 Classes: Wed & Fri 4:30pm - 5:50pm. <a href="https://campus-map.stanford.edu/?srch=200-203">Bldg 200 (Lane History Corner), Room 203</a>
<h2>Office Hours: Fri 2:00pm - 4:00pm (or by appointment) in Ashwin's office in ICME</h2>	
<h2>Course Assistant (CA): <a href="mailto:jeffgu@stanford.edu">Jeffrey Gu</a>.</h2>
<h2>Course Overview</h2>
<ul>
<li>Theory of Markov Decision Processes (MDP)</li>
<li>Dynamic Programming (DP) Algorithms</li>
<li>Reinforcement Learning (RL) Algorithms</li>
<li>We will model and apply these algorithms to 3 Financial/Trading problems:
<ul>
<li>(Dynamic) Asset-Allocation to maximize <em>Utility of Consumption</em></li>
<li>Optimal Exercise/Stopping of Path-dependent American Options</li>
<li> Optimal Trade Order Execution (managing <em>Price Impact</em>)</li>
</ul>
</li>
<li>By treating each of these problems as MDPs (i.e., Stochastic Control)</li>
<li>We will first go over classical/analytical solutions to these problems (in simple settings)</li>
<li>Then we will introduce real-world considerations, and tackle with RL (or DP)</li>
<li>Emphasis on Python implementations of these models and algorithms</li>
<li>More details on the course contents <a href="course_details.pdf">here</a></li>
</ul>
<h2>Pre-requisites</h2>
<ul>
<li> <strong>Required</strong>: Undergraduate-level background in Applied Mathematics, especially in Linear Algebra, Probability Theory and Optimization</li>
<li> <strong>Required</strong>: Background in Data Structures & Algorithms, with programming experience in numpy/scipy</li>
<li> <strong>Recommended</strong>: Basic background in Pricing and Portfolio Theory, although we will do an overview of the requisite Finance/Economics</li>
<li> <strong>Not required</strong>: MDP, DP, RL (we will cover these topics from scratch)</li>
</ul>
<h2>Grade will be based on</h2>
<ul>
	<li>20% Mid-Term Exam</li>
	<li>35% Final Exam</li>
	<li>35% Programming and Technical Writing Assignments (to be done throughout the course)</li>
	<li>10% Class Participation & Discussions on code/theory during office hours or online</li>
</ul>
<h2>Learning Material will be a combination of</h2>
<ul>
	<li><a href="lecture_slides/">Technical Documents/Lecture Slides I have prepared specifically for this course</a></li>
	<li><a href="https://github.com/coverdrive/MDP-DP-RL">Python codebase I have developed for this course</a> to help you "learn through coding"</li>
	<li><a href="http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching.html">Slides and Videos from David Silver's UCL course on RL</a></li>
	<li>For deeper self-study and reference, augment the above content with <a href="http://incompleteideas.net/book/the-book-2nd.html">The Sutton-Barto RL Book and Sutton's accompanying teaching material</a></li>
</ul>

<h2>Lecture-by-Lecture (tentative) schedule with corresponding lecture slides, reading/videos, and assignments</h2>
<ul>
	<li>January 9: <a href="course_details.pdf">Course Overview - Intro to Markov Decision Processes, Reinforcement Learning and Finance Application Problems</a></li>
	<ul>
		<li>Reading: First (Introduction) chapter of Sutton-Barto book (pages 1-12), <a href="lecture_slides/rich_sutton_slides/1-admin-and-intro.pdf">Sutton's slides on Intro to RL</a></li>
		<li>Assignment: 
			<ul>
				<li>Install/Setup on your laptop with LaTeX, Python 3 (and optionally Jupyter notebook)</li>
				<li>Create a git repo for this course where you can upload and organize all the code and technical writing you as part of assignments and self-learning</li>
				<li>Let the Course Assistant (CA) know of your git repo, so we can periodically review your assignments and other self-learning work</li>
			</ul>
		</li>
	</ul>
	<li>January 11: <a href="lecture_slides/david_silver_slides/MDP.pdf">Markov Decision Processes</a></li>
	<ul>
		<li>Reading: Third (MDP) chapter of Sutton-Barto book (pages 47-67), <a href="lecture_slides/rich_sutton_slides/5-6-MDPs.pdf">Sutton's slides on MDPs</a></li>
		<li>Video: <a href="https://www.youtube.com/watch?v=lfHX2hHRMVQ">David Silver's video (on youtube) on MDPs</a></li>
		<li>Assignment:
			<ul>
				<li>Write out the MP/MRP/MDP/Value Function definitions (in LaTeX) in your own style/notation (so you really internalize these concepts)</li>
				<li>Think about the data structures/class design (in Python 3) to represent MP/MRP/MDPs/Policies and implement them with clear type declarations</li>
				<li>Remember your data structure/code design must resemble the Mathematical/notational formalism as much as possible</li>
				<li>Separately implement the r(s,s') and the R(s) = \sum_{s'} p(s,s') r(s,s') definitions of MRP (likewise for MDP)</li>
				<li>Write code to convert/cast the former definition of MRP/MDP to the latter definition (put some thought into code design here)</li>
				<li>Write code to implement several key transformations/calculations you learnt in this lecture, eg:</li>
				<ul>
					<li>Generate the stationary distribution for an MP</li>
					<li>Create a MRP from an MDP and a (stochastic) Policy</li>
					<li>Calculate the Value function for an MRP that you learnt in this lecture (matrix inversion-based)</li>
				</ul>
			</ul>
		</li>
	</ul>

	<li>January 16: <a href="lecture_slides/david_silver_slides/MDP.pdf">Bellman Equations</a></li>
	<ul>
		<li>Assignment:
			<ul>
				<li>Write out all 8 Bellman Equations and also the transformation from Optimal Action-Value function to Optimal Policy (in LaTeX)</li>
				<li>Write code corresponding to the above equations</li>
			</ul>
		</li>
	</ul>
	<li>January 18: <a href="lecture_slides/david_silver_slides/DP.pdf">Dynamic Programming Algorithms</a></li>
	<ul>
		<li>Reading: Fourth (MDP) chapter of Sutton-Barto book, <a href="lecture_slides/rich_sutton_slides/7-8-DP.pdf">Sutton's slides on Dynamic Programming</a></li>
		<li>Video: <a href="https://www.youtube.com/watch?v=Nd1-UUMVfz4&t=3110s">David Silver's video (on youtube) on Dynamic Programming</a></li>
		<li>Assignment: Write code for Dynamic Programming Algorithms (Policy Iteration, Value Iteration) and Approximate Dynamic Programming Algorithms</li>
	</ul>
	<li>January 23: <a href="lecture_slides/UtilityTheoryForRisk.pdf">Understanding Risk-Aversion through Utility Theory</a>(as a pre-req to Application Problem 1)</li>
	<ul>
		<li>Optional (Related) reading: <a href="lecture_slides/EfficientFrontier.pdf">A Terse Introduction to Efficient Frontier Mathematics</a></li>
		<<li>Assignment:
			<ul>
				<li>Work out (in LaTeX) the equations for Absolute/Relative Risk Premia for CARA/CRRA respectively</li>
				<li>Write the Portfolio application problem statement and solution with precise notation (in LaTeX)</li> 
			</ul>
		</li>
	</ul>
	<li>January 25: <a href="lecture_slides/MertonPortfolio.pdf">Application Problem 1 - Optimal Asset Allocation/Consumption (Merton's 1969 Portfolio Problem)</a></li>
	<ul>
		<li>Reading: <a href="lecture_slides/PortfolioOptNotes.pdf">Discrete-time CARA example</a></li>
		<li>Optional Reading: <a href="StochasticCalculusFoundations.pdf">Stochastic Calculus Foundations</a>(used in setting up HJB)</li>
		<li>Assignment:
			<ul>
				<li>Model Merton's Portfolio problem as an MDP (write the model in LaTeX)</li>
				<li>Implement this MDP model in code</li>
				<li>Try recovering the closed-form solution with a DP algorithm that you implemented previously</li>
			</ul>
		</li>
	</ul>
	<li>January 30: Application Problem 1 - Optimal Asset Allocation/Consumption (Discussion on Real-World modeling and coding)</li>
	<li>February 1: <a href="lecture_slides/AmericanOptionsRL.pdf">Application Problem 2 - Optimal Exercise of American Options</a></li>
	<li>February 6: <a href="lecture_slides/OrderExecution.pdf">Application Problem 3 - Optimal Trade Order Execution</a></li>
	<li>February 8: Mid-Term Exam</li>
	<li>February 13 and 15: <a href="lecture_slides/david_silver_slides/MC-TD.pdf">Model-free Prediction (RL for Value Function Estimation)</a></li>
	<li>February 20 and 22: <a href="lecture_slides/david_silver_slides/control.pdf">Model-free Control (RL for Optimal Value Function/Policy)</a></li>
	<li>February 27 and March 1: <a href="lecture_slides/david_silver_slides/FA.pdf">RL with Function Approximation (including Deep RL)</a></li>
	<li>March 6 and March 8: <a href="lecture_slides/ValueFunctionGeometry.pdf">Batch Methods (DQN, LSTDQ/LSPI), and Gradient TD</a></li>
	<li>March 13 and 15: <a href="lecture_slides/PolicyGradient.pdf">Policy Gradient Algorithms</a></li>
	<li>March 22 (3:30pm - 6:30pm): Final Exam</li>
</ul>
<h2>Learning Tenets</h2>
<ul>
	<li>When learning Theory, blend notational rigor with intuitive understanding</li>
	<li>Write down the mathematical formalism of the lecture contents with great precision (without refering to how I've written them)</li>
	<li>Always start with a simple model/simple algorithm and be clear about why you might want to use a more complex model/algorithm</li>
	<li>Programming is a powerful tool to grasp mathematical concepts (code all ideas/models/algorithms with the aim to visualize as well as validate mathematical properties)</li>
	<li>Code modularity/re-use is important (leverage Object-Oriented and Functional Programming paradigms)</li>
	<li>Important to recognize and then model various frictions encountered in real-world trading</li>
	<li>Discuss the ideas you learn with other students, with the CA, and with me. Question everything!</li>
	<li>Recognize that this is an introductory course. Learn the foundations slowly and thoroughly, before graduating to advanced RL (other courses we offer at Stanford)</li>
</ul>
</body>
</html>

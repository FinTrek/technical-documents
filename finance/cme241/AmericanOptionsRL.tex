%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Beamer Presentation
% LaTeX Template
% Version 1.0 (10/11/12)
%
% This template has been downloaded from:
% http://www.LaTeXTemplates.com
%
% License:
% CC BY-NC-SA 3.0 (http://creativecommons.org/licenses/by-nc-sa/3.0/)
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%----------------------------------------------------------------------------------------
%	PACKAGES AND THEMES
%----------------------------------------------------------------------------------------

\documentclass[handout]{beamer}

\mode<presentation> {

% The Beamer class comes with a number of default slide themes
% which change the colors and layouts of slides. Below this is a list
% of all the themes, uncomment each in turn to see what they look like.

%\usetheme{default}
%\usetheme{AnnArbor}
%\usetheme{Antibes}
%\usetheme{Bergen}
%\usetheme{Berkeley}
%\usetheme{Berlin}
%\usetheme{Boadilla}
%\usetheme{CambridgeUS}
%\usetheme{Copenhagen}
%\usetheme{Darmstadt}
%\usetheme{Dresden}
%\usetheme{Frankfurt}
%\usetheme{Goettingen}
%\usetheme{Hannover}
%\usetheme{Ilmenau}
%\usetheme{JuanLesPins}
%\usetheme{Luebeck}
\usetheme{Madrid}
%\usetheme{Malmoe}
%\usetheme{Marburg}
%\usetheme{Montpellier}
%\usetheme{PaloAlto}
%\usetheme{Pittsburgh}
%\usetheme{Rochester}
%\usetheme{Singapore}
%\usetheme{Szeged}
%\usetheme{Warsaw}

% As well as themes, the Beamer class has a number of color themes
% for any slide theme. Uncomment each of these in turn to see how it
% changes the colors of your current slide theme.

%\usecolortheme{albatross}
%\usecolortheme{beaver}
%\usecolortheme{beetle}
%\usecolortheme{crane}
%\usecolortheme{dolphin}
%\usecolortheme{dove}
%\usecolortheme{fly}
%\usecolortheme{lily}
%\usecolortheme{orchid}
%\usecolortheme{rose}
%\usecolortheme{seagull}
%\usecolortheme{seahorse}
%\usecolortheme{whale}
%\usecolortheme{wolverine}

%\setbeamertemplate{footline} % To remove the footer line in all slides uncomment this line
%\setbeamertemplate{footline}[page number] % To replace the footer line in all slides with a simple slide count uncomment this line

%\setbeamertemplate{navigation symbols}{} % To remove the navigation symbols from the bottom of all slides uncomment this line
}

\usepackage{graphicx} % Allows including images
\usepackage{booktabs} % Allows the use of \toprule, \midrule and \bottomrule in tables
\usepackage{cool}
\usepackage{tikz}
\usepackage{amsmath}
\usepackage{pseudocode}
\usepackage{MnSymbol,wasysym}
\DeclareMathOperator*{\argmax}{argmax}
\DeclareMathOperator*{\argmin}{argmin}
\usetikzlibrary{positioning}

%----------------------------------------------------------------------------------------
%	TITLE PAGE
%----------------------------------------------------------------------------------------

\title[RL for American Options]{Pricing American Options with Reinforcement Learning} % The short title appears at the bottom of every slide, the full title is only on the title page

\author{Ashwin Rao} % Your name
\institute[Stanford] % Your institution as it will appear on the bottom of every slide, may be shorthand to save space
{
ICME, Stanford University
 % Your institution for the title page
}

\date{\today} % Date, can be changed to a custom date

\begin{document}
\begin{frame}
\titlepage % Print the title page as the first slide
\end{frame}

\begin{frame}
\frametitle{Overview} % Table of contents slide, comment this block out to remove it
\tableofcontents % Throughout your presentation, if you choose to use \section{} and \subsection{} commands, these will automatically be printed on this slide as an overview of your presentation
\end{frame}

\section{Review of Optimal Stopping and its MDP Formulation}

\begin{frame}
\frametitle{Stopping Time}
\pause
\begin{itemize}[<+->]
\item Stopping time $\tau$ is a ``random time'' (random variable) interpreted as time at which a given stochastic process exhibits certain behavior
\item Stopping time often defined by a ``stopping policy'' to decide whether to continue/stop a process based on present position and past events
\item Random variable $\tau$ such that $Pr[\tau \leq t]$ is in $\sigma$-algebra $\mathcal{F}_t$, for all $t$
\item Deciding whether $\tau \leq t$ only depends on information up to time $t$
\item Hitting time of a Borel set $A$ for a process $X_t$ is the first time $X_t$ takes a value within the set $A$
\item Hitting time is an example of stopping time. Formally, 
$$T_{X,A} = \min \{t \in \mathbb{R} | X_t \in A\}$$
eg: Hitting time of a process to exceed a certain fixed level
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Optimal Stopping Problem}
\pause
\begin{itemize}[<+->]
\item Optimal Stopping problem for Stochastic Process $X_t$: 
$$V(x) = \max_{\tau} \mathbb{E}[G(X_{\tau})|X_0 = x]$$
 where $\tau$ is a set of stopping times of $X_t$, $V(\cdot)$ is called the Value function, and $G$ is the Reward function.
\item Note that sometimes we can have several stopping times that maximize $\mathbb{E}[G(X_{\tau})]$ and we say that the optimal stopping time
is the smallest stopping time achieving the maximum value.
\item Example of Optimal Stopping: Optimal Exercise of American Options
\begin{itemize}
\item $X_t$ is stochastic process for underlying security's price
\item $x$ is underlying security's current price
\item $\tau$ is set of exercise times corresponding to various stopping policies
\item $V(\cdot)$ is American option price as function of underlying's current price
\item $G(\cdot)$ is the option payoff function, adjusted for time-discounting
\end{itemize}
\end{itemize}
\end{frame}


\begin{frame}
\frametitle{Optimal Stopping Problems as Markov Decision Processes}
\pause
\begin{itemize}[<+->]
\item We formulate Stopping Time problems as Markov Decision Processes
\item {\em State} is $X_t$
\item {\em Action} is Boolean: Stop or Continue
\item {\em Reward} always 0, except upon Stopping (when it is $=G(X_{\tau})$)
\item {\em State}-transitions governed by the Stochastic Process $X_t$
\item For discrete time steps, the Bellman Optimality Equation is:
$$V^*(X_t) = \max(G(X_t), \mathbb{E}[V^*(X_{t+1})|X_t])$$
\item For finite number of time steps, we can do a simple backward induction algorithm from final time step back to time step 0
\end{itemize}
\end{frame}

\section{Longstaff-Schwartz Algorithm}

\begin{frame}
\frametitle{Mainstream approaches to American Option Pricing}
\pause
\begin{itemize}[<+->]
\item American Option Pricing is Optimal Stopping, and hence an MDP
\item So can be tackled with Dynamic Programming or RL algorithms
\item But let us first review the mainstream approaches
\item For some American options, just price the European, eg: vanilla call
\item When payoff is not path-dependent and state dimension is not large, we can do backward induction on a binomial/trinomial tree/grid
\item Otherwise, the standard approach is \href{https://people.math.ethz.ch/~hjfurrer/teaching/LongstaffSchwartzAmericanOptionsLeastSquareMonteCarlo.pdf}{\underline{\textcolor{blue}{Longstaff-Schwartz algorithm}}}
\item Longstaff-Schwartz algorithm combines 3 ideas:
\begin{itemize}
\item Valuation based on Monte-Carlo simulation
\item Function approximation of continuation value for in-the-money states
\item Backward-recursive determination of early exercise states
\end{itemize}
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Ingredients of Longstaff-Schwartz Algorithm}
\pause
\begin{itemize}[<+->]
\item $m$ Monte-Carlo paths indexed $i = 0, 1, \ldots, m-1$
\item $n+1$ time steps indexed $j = n, n-1, \ldots, 1, 0$ (we move back in time)
\item Infinitesimal Risk-free rate at time $t_j$ denoted $r_{t_j}$
\item Simulation paths of prices of underlying as input 2-dim array $SP[i,j]$
\item At each time step, $CF[i]$ is PV of current+future cashflow for path $i$
\item $s_{i,j}$ denotes state for $(i,j)$ $:=$ (time $t_j$, price history $SP[i,:(j+1)]$)
\item $Payoff(s_{i,j})$ denotes Option Payoff at $(i,j)$
\item $\phi_0(s_{i,j}), \ldots, \phi_{r-1}(s_{i,j})$ represent feature functions (of state $s_{i,j}$)
\item $w_0, \ldots, w_{r-1}$ are the regression weights
\item Regression function $f(s_{i,j}) = w \cdot \phi(s_{i,j}) = \sum_{l=0}^{r-1} w_l \cdot \phi_l(s_{i,j})$
\item $f(\cdot)$ is estimate of continuation value for in-the-money states
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{The Longstaff-Schwartz Algorithm}
\pause
\begin{pseudocode}{LongstaffSchwartz}{SP[0:m, 0:n+1]}
\COMMENT{$s_{i,j}$ is shorthand for state at $(i,j) :=$ ($t_j$, $SP[i,:(j+1)]$)}\\
CF[0:m] \GETS [Payoff(s_{i,n}) \mbox{ for } i \mbox{ in range}(m)]\\
\FOR j \GETS n - 1 \DOWNTO 1 \DO
\BEGIN
CF[0:m] \GETS CF[0:m] * e^{-r_{t_j}(t_{j+1} - t_j)}\\
X \GETS [\phi(s_{i,j}) \mbox{ for } i \mbox{ in range}(m) \mbox{ if } Payoff(s_{i,j}) > 0]\\
Y \GETS [CF[i] \mbox{ for } i \mbox{ in range}(m) \mbox{ if } Payoff(s_{i,j}) > 0]\\
w \GETS (X^T \cdot X)^{-1} \cdot X^T \cdot Y\\
\COMMENT{Above regression gives estimate of continuation value}\\
\FOR i \GETS 0 \TO m-1 \DO
CF[i] \GETS Payoff(s_{i,j}) \mbox{\bf{ if }}  Payoff(s_{i,j}) > w \cdot \phi(s_{i,j})
\END
\\
exercise \GETS Payoff(s_{0,0})\\
continue \GETS e^{-r_0(t_1-t_0)} \cdot mean(CF[0:m])\\
\RETURN{\max(exercise, continue)}\\
\end{pseudocode}
\end{frame}

\section{RL Algorithms for American Option Pricing: LSPI and FQI}

\begin{frame}
\frametitle{RL as an alternative to Longstaff-Schwartz}
\pause
\begin{itemize}[<+->]
\item RL is straightforward if we clearly define the MDP
\item {\em State} is [Current Time, History of Underlying Security Prices]
\item {\em Action} is Boolean: Exercise (i.e., Stop) or Continue
\item {\em Reward} always 0, except upon Exercise ($=$ Payoff)
\item {\em State}-transitions governed by Underlying Price's Stochastic Process
\item Key is function approximation of state-conditioned continuation value
\item Continuation Value $\Rightarrow$ Optimal Stopping $\Rightarrow$ Option Price
\item We outline two RL Algorithms:
\begin{itemize}
\item Least Squares Policy Iteration (LSPI)
\item Fitted Q-Iteration (FQI)
\end{itemize}
\item Both Algorithms are batch methods solving a linear system
\item More details in \href{http://proceedings.mlr.press/v5/li09d/li09d.pdf}{\underline{\textcolor{blue}{Li, Szepesvari, Schuurmans paper}}}
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Least Squares Policy Iteration for Continuation Value}
\pause
\begin{pseudocode}{LSPI-AmericanPricing}{SP[0:m, 0:n+1]}
\COMMENT{$s_{i,j}$ is shorthand for state at $(i,j) :=$ ($t_j$, $SP[i,:(j+1)]$)}\\
\COMMENT{$A$ is an $r \times r$ matrix, $b$ and $w$ are $r$-length vectors}\\
\COMMENT{$A_{i,j} \leftarrow \phi(s_{i,j}) \cdot  (\phi(s_{i,j}) - \gamma \mathbb{I}_{w\cdot \phi(s_{i,j+1}) \geq Payoff(s_{i,j+1})} * \phi(s_{i,j+1}))^T$}\\
\COMMENT{$b_{i,j} \leftarrow \gamma \mathbb{I}_{w \cdot \phi(s_{i,j+1}) < Payoff(s_{i,j+1})} * Payoff(s_{i,j+1}) * \phi(s_{i,j})$}\\
A \GETS 0, B \GETS 0, w \GETS 0\\
\FOR i \GETS 0 \TO m - 1 \DO
\BEGIN
\FOR j \GETS 0 \TO n-1 \DO
\BEGIN
Q \GETS Payoff(s_{i,j+1})\\
P \GETS \phi(s_{i,j+1}) \mbox{ {\bf if }} j < n-1 \mbox{\bf{ and }} Q \leq w \cdot \phi(s_{i,j+1}) \mbox{\bf{ else} } 0\\
R \GETS Q \mbox{ {\bf if }} Q > w \cdot P \mbox{\bf{ else} } 0 \\
A \GETS A + \phi(s_{i,j}) \cdot (\phi(s_{i,j}) - e^{-r_{t_j}(t_{j+1}-t_j)} * P)^T\\
B \GETS B + e^{-r_{t_j}(t_{j+1}-t_j)} *  R * \phi(s_{i,j})\\
\END\\
w \GETS A^{-1} \cdot b, A \GETS 0, b \GETS 0 \mbox{\bf{ if }} (i+1) \% BatchSize == 0
\END\\
\end{pseudocode}
\end{frame}

\begin{frame}
\frametitle{Fitted Q-Iteration for Continuation Value}
\pause
\begin{pseudocode}{FQI-AmericanPricing}{SP[0:m, 0:n+1]}
\COMMENT{$s_{i,j}$ is shorthand for state at $(i,j) :=$ ($t_j$, $SP[i,:(j+1)]$)}\\
\COMMENT{$A$ is an $r \times r$ matrix, $b$ and $w$ are $r$-length vectors}\\
\COMMENT{$A_{i,j} \leftarrow \phi(s_{i,j}) \cdot \phi(s_{i,j})^T$}\\
\COMMENT{$b_{i,j} \leftarrow \gamma \max(Payoff(s_{i,j+1}), w \cdot \phi(s_{i,j+1})) * \phi(s_{i,j})$}\\
A \GETS 0, B \GETS 0, w \GETS 0\\
\FOR i \GETS 0 \TO m - 1 \DO
\BEGIN
\FOR j \GETS 0 \TO n-1 \DO
\BEGIN
Q \GETS Payoff(s_{i,j+1})\\
P \GETS \phi(s_{i,j+1}) \mbox{ {\bf if }} j < n-1 \mbox{\bf{ else} } 0\\
A \GETS A + \phi(s_{i,j}) \cdot \phi(s_{i,j})^T\\
B \GETS B + e^{-r_{t_j}(t_{j+1}-t_j)} *  \max(Payoff(s_{i,j+1}), w \cdot P) * \phi(s_{i,j})\\
\END\\
w \GETS A^{-1} \cdot b, A \GETS 0, b \GETS 0 \mbox{\bf{ if }} (i+1) \% BatchSize == 0
\END\\
\end{pseudocode}
\end{frame}

\section{Choice of feature functions and Model-based adaptation}

\begin{frame}
\frametitle{Feature functions}
\pause
\begin{itemize}[<+->]
\item Li, Szepesvari, Schuurmans recommend Laguerre polynomials (first 3)
\item Over $S' = S_t/K$ where $S_t$ is underlying price and $K$ is strike
\item $\phi_0(S_t) = 1, \phi_1(S_t) = e^{-\frac {S'} 2}, \phi_2(S_t) = e^{-\frac{S'} 2} \cdot (1-S'), \phi_3(S_t) = e^{-\frac{S'} 2} \cdot (1-2S'+S'^2/2)$
\item They used these for Longstaff-Schwartz as well as for LSPI and FQI
\item For LSPI and FQI, we also need feature functions for time
\item They recommend $\phi_0^t(t) = sin(\frac {\pi(T-t)} {2T}), \phi_1^t(t) = \log(T-t), \phi_2^t(t) = (\frac t T)^2$
\item They claim LSPI and FQI perform better than Longstaff-Schwartz with this choice of features functions
\item We will code up these algorithms to validate this claim \smiley{}
\end{itemize}
\end{frame}



\end{document}
\documentclass[10pt]{amsart}
\usepackage{geometry} % see geometry.pdf on how to lay out the page. There's lots.
\geometry{a4paper} % or letter or a5paper or ... etc
% \geometry{landscape} % rotated page geometry

\usepackage[toc,page]{appendix}
% See the ``Article customise'' template for come common customisations

\usepackage{graphicx}	
\usepackage{listings}
\usepackage{cool}
\usepackage{amsmath}
\DeclareMathOperator*{\argmin}{arg\,min}

\title{Moment Generating Function and it's Applications}
\author{Ashwin Rao}
\date{} % delete this line to display the current date

%%% BEGIN DOCUMENT
\begin{document}

\maketitle

The purpose of this note is to introduce the {\em Moment Generating Function (MGF)} and demonstrate it's utility in several applications in Applied Mathematics. 

\section{The Moment Generating Function (MGF)}
The Moment Generating Function (MGF) of a random variable $x$ (discrete or continuous) is defined as a function $f_x : \mathbb{R} \rightarrow \mathbb{R}^+$ such that:

\begin{equation}
f_x(t) = \mathbb{E}_x[e^{tx}] \mbox{ for all } t \in \mathbb{R} \label{eq:mgfdef}
\end{equation}

Let us denote the $n^{th}$-derivative of $f_x$ as $f_x^{(n)} : \mathbb{R} \rightarrow \mathbb{R}$ for all $n\in \mathbb{Z}_{\geq 0}$ ($f_x^{(0)}$ is defined to be simply the MGF $f_x$).

\begin{equation}
f_x^{(n)}(t) = \mathbb{E}_x[x^n \cdot e^{tx}] \mbox{ for all } n\in \mathbb{Z}_{\geq 0} \mbox{ for all } t\in \mathbb{R} \label{eq:mgfderiv}
\end{equation}

\begin{equation}
f_x^{(n)}(0) = \mathbb{E}_x[x^n]  \label{eq:derivat0}
\end{equation}

\begin{equation}
f_x^{(n)}(1) = \mathbb{E}_x[x^n \cdot e^x]  \label{eq:derivat1}
\end{equation}

Equation (\ref{eq:derivat0}) tells us that $f^{(n)}(0)$ gives us the $n^{th}$ moment of $x$. In particular, $f_x^{(1)}(0) = f'(0)$ gives us the mean and $f_x^{(2)}(0) - (f_x^{(1)}(0))^2 = f_x''(0) - (f_x'(0))^2$ gives us the variance. Note that this holds true for any distribution for $x$. This is rather convenient since all we need is the functional form for the distribution of $x$. This would lead us to the expression for the MGF (in terms of $t$). Then, we take derivatives of this MGF and evaluate those derivatives at 0 to obtain the moments of $x$.

Equation (\ref{eq:derivat1}) helps us calculate the often-appearing expectation $\mathbb{E}_x[x^n \cdot e^x]$. In fact, $\mathbb{E}_x[e^x]$ and $\mathbb{E}_x[x \cdot e^x]$ are very common in several areas of Applied Mathematics. Again, note that this holds true for any distribution for $x$.

\section{MGF for the Normal Distribution}
Here we assume that the random variables $x$ follows a normal distribution. Let $x \sim \mathcal{N}(\mu, \sigma^2)$.

\begin{align}
f_{x\sim \mathcal{N}(\mu, \sigma^2)}(t) & = \mathbb{E}_{x\sim \mathcal{N}(\mu, \sigma^2)}[e^{tx}] \\
& = \int_{-\infty}^{+\infty} \frac {1} {\sqrt{2\pi} \sigma} \cdot e^{-\frac {(x - \mu)^2} {2\sigma^2}} \cdot e^{tx} \cdot dx \\
& = \int_{-\infty}^{+\infty} \frac {1} {\sqrt{2\pi} \sigma} \cdot e^{-\frac {(x-(\mu +t\sigma^2))^2} {2\sigma^2}} \cdot e^{\mu t + \frac {\sigma^2 t^2} {2}} \cdot dx \\
& = e^{\mu t + \frac {\sigma^2 t^2} 2} \cdot \mathbb{E}_{x\sim \mathcal{N}(\mu + t\sigma^2, \sigma^2)}[1] \\
& = e^{\mu t + \frac {\sigma^2 t^2} 2}
\end{align}

\begin{equation}
f_{x\sim \mathcal{N}(\mu, \sigma^2)}'(t) = \mathbb{E}_{x\sim \mathcal{N}(\mu, \sigma^2)}[x \cdot e^{tx}] = (\mu + \sigma^2t)\cdot e^{\mu t + \frac {\sigma^2 t^2} 2} \label{eq:normmgfderiv}
\end{equation}
\begin{equation}
f_{x\sim \mathcal{N}(\mu, \sigma^2)}''(t) = \mathbb{E}_{x\sim \mathcal{N}(\mu, \sigma^2)}[x \cdot e^{tx}] = ((\mu + \sigma^2t)^2 + \sigma^2)\cdot e^{\mu t + \frac {\sigma^2 t^2} 2} \label{eq:normmgfdoublederiv}
\end{equation}
\begin{equation}
f_{x\sim \mathcal{N}(\mu, \sigma^2)}'(0) = \mathbb{E}_{x\sim \mathcal{N}(\mu, \sigma^2)}[x] = \mu
\end{equation}
\begin{equation}
f_{x\sim \mathcal{N}(\mu, \sigma^2)}''(0) = \mathbb{E}_{x\sim \mathcal{N}(\mu, \sigma^2)}[x] = \mu^2 + \sigma^2
\end{equation}
\begin{equation}
f_{x\sim \mathcal{N}(\mu, \sigma^2)}'(1) = \mathbb{E}_{x\sim \mathcal{N}(\mu, \sigma^2)}[x\cdot e^x] = (\mu + \sigma^2)e^{\mu+ \frac {\sigma^2} 2}
\end{equation}
\begin{equation}
f_{x\sim \mathcal{N}(\mu, \sigma^2)}''(1) = \mathbb{E}_{x\sim \mathcal{N}(\mu, \sigma^2)}[x\cdot e^x] = ((\mu + \sigma^2)^2 + \sigma^2)e^{\mu+ \frac {\sigma^2} 2}
\end{equation}

\section{Minimizing the MGF}
Now let us consider the problem of minimizing the MGF. The problem is to:
$$\min_{t\in \mathbb{R}} f_x(t) = \min_{t\in \mathbb{R}} \mathbb{E}_x[e^{tx}]$$

This problem of minimizing $\mathbb{E}_x[e^{tx}]$ shows up a lot in various places in Applied Mathematics when dealing with exponential functions (eg: when optimizing the Expectation of a Constant Absolute Risk-Aversion Utility function $U(y) = \frac {-e^{-\gamma y}} {\gamma}$ where $\gamma$ is the coefficient of risk-aversion and where $y$ is a parameterized function of a random variable $x$).

Let us denote $t^*$ as the value of $t$ that minimizes the MGF. Specifically,
$$t^* = \argmin_{t\in \mathbb{R}} f_x(t) = \argmin_{t \in \mathbb{R}} \mathbb{E}_x[e^{tx}]$$

\subsection{Minimizing the MGF when $x$ follows a normal distribution}
Here we consider the fairly typical case where $x$ follows a normal distribution. Let $x\sim \mathcal{N}(\mu, \sigma^2)$. Then we have to solve the problem:
$$\min_{t\in \mathbb{R}} f_{x\sim \mathcal{N}(\mu, \sigma^2)}(t) = \min_{t\in \mathbb{R}} \mathbb{E}_{x\sim \mathcal{N}(\mu, \sigma^2)}[e^{tx}] = \min_{t\in \mathbb{R}} e^{\mu t + \frac {\sigma^2 t^2} 2}$$
From Equation (\ref{eq:normmgfderiv}) above, we have:
$$f_{x\sim \mathcal{N}(\mu, \sigma^2)}'(t) = (\mu + \sigma^2t)\cdot e^{\mu t + \frac {\sigma^2 t^2} 2}$$
Setting this to 0 yields:
$$(\mu + \sigma^2t^*)\cdot e^{\mu t^* + \frac {\sigma^2 {t^*}^2} 2} = 0$$
which leads to:
\begin{equation}
t^* = \frac {-\mu} {\sigma^2}
\end{equation}
From Equation (\ref{eq:normmgfdoublederiv}) above, we have:
$$f_{x\sim \mathcal{N}(\mu, \sigma^2)}''(t) = ((\mu + \sigma^2t)^2 + \sigma^2)\cdot e^{\mu t + \frac {\sigma^2 t^2} 2} > 0 \mbox{ for all } t \in \mathbb{R}$$
which confirms that $t^*$ is a minima.

Substituting $t=t^*$ in $f_{x\sim \mathcal{N}(\mu, \sigma^2)}(t) = e^{\mu t + \frac {\sigma^2 t^2} 2}$ yields:
\begin{equation}
\min_{t\in \mathbb{R}} f_{x\sim \mathcal{N}(\mu, \sigma^2)}(t) = e^{\mu t^* + \frac {\sigma^2 {t^*}^2} 2} = e^{\frac {-\mu^2} {2\sigma^2}}
\end{equation}

\subsection{Minimizing the MGF when $x$ is a symmetric binary distribution}
Here we consider the case where $x$ follows a binary distribution: $x$ takes values $\mu + \sigma$ and $\mu - \sigma$ with probability 0.5 each. Let us refer to this distribution as $x \sim \mathcal{B}(\mu + \sigma, \mu - \sigma)$. Note that the mean and variance of $x$ under $\mathcal{B}(\mu + \sigma, \mu - \sigma)$ are $\mu$ and $\sigma^2$ respectively. So we have to solve the problem:
$$\min_{t\in \mathbb{R}} f_{x\sim \mathcal{B}(\mu + \sigma, \mu - \sigma)}(t) = \min_{t\in \mathbb{R}} \mathbb{E}_{x\sim \mathcal{B}(\mu + \sigma, \mu - \sigma)}[e^{tx}] = \min_{t\in \mathbb{R}} 0.5(e^{(\mu + \sigma)t} + e^{(\mu - \sigma)t})$$
$$f_{x\sim \mathcal{B}(\mu + \sigma, \mu - \sigma)}'(t) = 0.5((\mu + \sigma) \cdot e^{(\mu + \sigma)t} + (\mu - \sigma) \cdot e^{(\mu - \sigma)t})$$
Note that unless $\mu \in \mbox{ open interval } (-\sigma, \sigma) $ (i.e., absolute value of mean is less than standard deviation), $f_{x\sim \mathcal{B}(\mu + \sigma, \mu - \sigma)}'(t)$ will not be 0 for any value of $t$. Therefore, for this minimization to be non-trivial, we will henceforth assume $\mu \in (-\sigma, \sigma)$.
With this assumption in place, setting $f_{x\sim \mathcal{B}(\mu + \sigma, \mu - \sigma)}'(t)$ to 0 yields:
$$(\mu + \sigma) \cdot e^{(\mu + \sigma)t^*} + (\mu - \sigma) \cdot e^{(\mu - \sigma)t^*} = 0$$
which leads to:
\begin{equation}
t^* = \frac 1 {2\sigma} \ln{(\frac {\sigma - \mu} {\mu + \sigma})}
\end{equation}
Note that
$$f_{x\sim \mathcal{B}(\mu + \sigma, \mu - \sigma)}''(t) =  0.5((\mu + \sigma)^2 \cdot e^{(\mu + \sigma)t} + (\mu - \sigma)^2 \cdot e^{(\mu - \sigma)t}) > 0 \mbox{ for all } t \in \mathbb{R}$$
which confirms that $t^*$ is a minima.

Substituting $t=t^*$ in $f_{x\sim \mathcal{B}(\mu + \sigma, \mu - \sigma)}(t) = 0.5(e^{(\mu + \sigma)t} + e^{(\mu - \sigma)t})$ yields:
\begin{equation}
\min_{t\in \mathbb{R}} f_{x\sim \mathcal{B}(\mu + \sigma, \mu - \sigma)}(t) = 0.5(e^{(\mu + \sigma)t^*} + e^{(\mu - \sigma)t^*}) = 0.5((\frac {\sigma - \mu} {\mu+ \sigma})^{\frac {\mu + \sigma} {2\sigma}} + (\frac {\sigma - \mu} {\mu+ \sigma})^{\frac {\mu - \sigma} {2\sigma}})
\end{equation}


\section{More Applications coming up soon ...}


\end{document}